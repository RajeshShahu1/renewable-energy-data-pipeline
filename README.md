

# Renewable Energy Dat End-to-End Pipeline Using AWS Services

This project showcases a fully integrated, real-time energy data pipeline built using AWS services such as S3, Lambda, DynamoDB, CloudWatch, AWS CLI and SNS. It leverages Python and FastAPI to enable API-driven access and insightful visualizations, providing a robust and scalable solution for monitoring and analyzing renewable energy data.

---

## Project Overview

The objective of this project is to build a real-time data pipeline that simulates energy data from multiple renewable energy sites. The system ingests and processes data using AWS Lambda, stores it in DynamoDB, detects anomalies, and sends real-time alerts via SNS. It also includes API access for querying data and generates insightful visualizations for monitoring trends and performance.

**The project leverages the following tools and services:**

- **Python**: Simulated live data feed generation 
- **Amazon S3**:File storage and event-based trigger for ingestion
- **AWS Lambda**: Real-time processing and anomaly detection
- **DynamoDB**: Scalable NoSQL storage for processed data
- **FastAPI**: Lightweight REST API framework for querying and alerts
- **Amazon SNS**: Anomaly detection & real-time alerts
- **Seaborn/Matplotlib/Plotly**: Static data visualizations and trends 
- **Amazon CloudWatch**: Centralized logging and monitoring for Lambda
- **AWS CLI**: Infrastructure provisioning and automation (IaC)
- **GitHub Actions**: CI/CD automation for Lambda deployments

---

## Architecture
![Architecture.png](Architecture.png)

---

## Setup Instructions

### Prerequisites

* AWS Account (with admin or necessary permissions for S3, DynamoDB, Lambda, IAM, AmazonSNS, CloudWatch.)
* Download and install AWS CLI from https://aws.amazon.com/cli/
* Python 3.8 or later https://www.python.org/downloads/
* download and install Git from https://git-scm.com/downloads
* `virtualenv` installed
* Note: This project uses AWS Free Tier resources
  
### Configuration of AWS
Open your command prompt or terminal and run the following command:
```bash
   aws configure
```
When prompted, provide your AWS credentials:

* Enter your AWS Access Key ID
* Enter your AWS Secret Access Key
* Set the default region as us-east-1
* Choose json as the default output format

### Local Environment Setup

```bash
# Clone the repository
git clone https://github.com/RajeshShahu1/renewable-energy-data-pipeline.git
cd renewable-energy-data-pipeline

# Create and activate virtual environment
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows

# Install dependencies
pip install -r requirements.txt
```
### Infrastructure Deployment (AWS CLI)

From the infrastructure/ folder, run:
```bash
   infrastructure_setup.sh
```
Ensure the following files are in this folder:

* trust-policy.json
* s3_event.json
* lambda_function.py

Resources created:

* S3 bucket for data ingestion
* DynamoDB table: energy_data
* Lambda function triggered by S3
* SNS topic for alerts
* IAM role and policies
  
### AWS Resource Setup

* **S3 Bucket** ('renewable-energy-data1') for simulated data uploads
* **DynamoDB Table** (`energy_data`)

  * Partition key: site_id(string) Unique identifier for each site
  * Sort key: timestamp(string) Time of the energy record
  * energy_generated_kwh (number) â€“ Energy generated by the site in kilowatt-hours
  * energy_consumed_kwh (number) â€“ Energy consumed by the site in kilowatt-hours
  * net_energy_kwh (number) â€“ Calculated as generated - consumed
  * anomaly (boolean) â€“ Flag to indicate anomaly 
* **SNS Topic** (`energy-anomaly-alerts`) with email subscription
* **IAM Role** with permissions: S3, DynamoDB, SNS, Lambda, CloudWatch
* **CloudWatch Logs** (automatically integrated with Lambda)

---

## How to Run Components

### Start Simulated Live Data Feed

Continuously uploads JSON files with energy data to S3.
run:
```bash
cd data_generation
pip install -r requirements.txt
python simulated_data_feed.py
```

### Lambda Function

Triggered on every S3 upload. Performs:

* Parsing and validation JSON
* Anomaly detection 
* Storing processed records in DynamoDB
* Sending alert via SNS (if anomaly found)

### Run FastAPI Backend

```bash
cd APIs
pip install -r requirements.txt
uvicorn dynamodb_api_fastapi:app --reload
```

Access Swagger docs at:

```
http://localhost:8000/docs
```
### How to Use the API

### `/records`

Fetch records by site and optional time range.

```
GET /records?site_id=SolarFarm_AZ_001&start_time=2025-06-06T00:00:00&end_time=2025-06-06T23:59:59
```
### `/anomalies`

Fetch anomalies for a specific site.

```
GET /anomalies?site_id=BatteryBank_TX_005
```
The FastAPI backend runs locally on http://localhost:8000 when launched using Uvicorn./records
returns energy data (generated, consumed, net) for a given site_id within an optional time range./anomalies returns only records with anomalies (e.g., negative or unusually high values) for a specific site_id. 

---

## How to Visualize Data

Run the visualization script:

```bash
cd visualization
pip install -r requirements.txt
python visualization.py
```
---
## GitHub Setup & CI/CD Pipeline
Step 1: Initialize GitHub Repository
```bash
git init
git remote add origin https://github.com/RajeshShahu1/renewable-energy-data-pipeline.git
git add .
git commit -m "Initial commit"
git push -u origin main
```
Step 2: Set Up GitHub Secrets
In your GitHub repository:

Go to Settings > Secrets > Actions > New repository secret

Add the following secrets:

* AWS_ACCESS_KEY_ID
* AWS_SECRET_ACCESS_KEY
* AWS_REGION
  
Step 3: Configure CI/CD Workflow
Create a file: .github/workflows/deploy.yml
```bash
name: Deploy Lambda Function
on:
  push:
     -'lambda_function/**'
    branches:
      - main
jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Set up python
        uses: actions/setup-python@v4
        with:
          python-version: '3.13'
        name: Install dependencies
        working-directory: lambda_function
        run:
          python -m pip install --upgrade pip
          pip install -r requirements.txt -t .

        name: Zip Lambda code
        run: |
          cd lambda_function
          zip -r ../lambda_function.zip .
        - name: Deploy to AWS Lambda
          uses: appleboy/lambda-action@master
          with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: 'us-west-2'
          function_name: 'ProcessEnergyData'
          zip_file: 'lambda_function.zip'
```
Push changes to trigger deployment:
```bash
git add .
git commit -m "Trigger CI/CD pipeline"
git push origin main
```
---
## Monitoring & Debugging
```bash
# View uploaded files in S3
aws s3 ls s3://renewable-energy-data1 --recursive

# View recent records in DynamoDB
aws dynamodb scan --table-name energy_data --limit 5

# Stream Lambda logs from CloudWatch
aws logs tail /aws/lambda/renewable-energy-pipeline-lambda --follow

# Manually invoke the Lambda function
aws lambda invoke --function-name renewable-energy-pipeline-lambda output.json

# Check SNS topics
aws sns list-topics

# Check IAM role permissions
aws iam list-attached-role-policies --role-name renewable-energy-lambda-role
```
---
## Troubleshooting
* Lambda not triggering? Verify S3 event notifications and IAM permissions
* DynamoDB permission denied? Update Lambda IAM role
* API not responding? Check that FastAPI is running and no port conflicts exist
* No Visualization data? Confirm data is in DynamoDB and table name is correct
---
## Design Decisions

* **S3 â†’ Lambda â†’ DynamoDB**: Serverless and scalable data ingestion pattern.
* **SNS for alerts**: Enables real-time anomaly notification (email/SMS).
* **FastAPI**: Lightweight, fast API framework with built-in Swagger UI.
* **CloudWatch**: Logs every Lambda execution and captures errors.
* **DynamoDB**: Low-latency NoSQL storage for high-throughput access.
* **Seaborn/Matplotlib**: Easy and polished static chart generation.
---
## Cleanup Instructions
To delete AWS resources...
```bash
aws s3 rm s3://renewable-energy-data1 --recursive
aws s3 rb s3://renewable-energy-data1
aws dynamodb delete-table --table-name energy_data
aws lambda delete-function --function-name ProcessEnergyData
```
---
## Demo Video (to be added)

ðŸ“Ž Link to your video demonstration of the project
ðŸ§¾ Walkthrough of key features, decisions, and usage instructions

---

## Contact

Created by **\[Rajesh Shahu]**
ðŸ“§ Email: [rajshahu4446@gmail.com](mailto:rajshahu4446@gmail.com)
ðŸ”— [LinkedIn](https://linkedin.com/in/rajeshshahu)

---


